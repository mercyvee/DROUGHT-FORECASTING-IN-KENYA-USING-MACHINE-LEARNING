{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91103136",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/Admin/Desktop/PROJECT WORK/DROUGHT-FORECASTING-IN-KENYA-USING-MACHINE-LEARNING/MLFile3.xlsx\"\n",
    "sheet_names = pd.ExcelFile(file_path).sheet_names\n",
    "sheet_dict = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n",
    "print (sheet_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb45602-ab81-4b25-ab15-909aeb7bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION AND COMPOSITE INDICES\n",
    "\n",
    "filtered_sheet_dict = {}\n",
    "correlation_dict = {} \n",
    "\n",
    "for station, df in sheet_dict.items():\n",
    "    print(f\"\\nProcessing station: {station}\")\n",
    "    print(f\"Original columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy.dropna(inplace=True)\n",
    "    \n",
    "\n",
    "    if df_copy.empty:\n",
    "        print(f\"No data left for {station} after dropping missing values. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    year_col = \"YEAR\"\n",
    "    rainfall_col = station  \n",
    "    \n",
    "    print(f\"Year column: {year_col}\")\n",
    "    print(f\"Rainfall column: {rainfall_col}\")\n",
    "    \n",
    "    sst_cols = [col for col in df_copy.columns if col not in [year_col, rainfall_col]]\n",
    "    print(f\"SST columns: {sst_cols}\")\n",
    "    \n",
    "    print(f\"\\nCorrelation results for {station}:\")\n",
    "    \n",
    "    retained_cols = []\n",
    "    station_correlations = {}  # Store correlations for this station\n",
    "    \n",
    "    for col in sst_cols:\n",
    "        try:\n",
    "            corr, p_value = stats.pearsonr(df_copy[col], df_copy[rainfall_col])\n",
    "            print(f\"{col}: r={corr:.2f}, p={p_value:.3f}\")\n",
    "            station_correlations[col] = {'correlation': corr, 'p_value': p_value}  # Save correlation stats\n",
    "            \n",
    "            if p_value <= 0.05:\n",
    "                retained_cols.append(col)\n",
    "            else:\n",
    "                print(f\"Dropped {col} from {station} (p={p_value:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {col}: {str(e)}\")\n",
    "    \n",
    "    correlation_dict[station] = station_correlations\n",
    "    \n",
    "    print(f\"Retained {len(retained_cols)} predictors out of {len(sst_cols)} original variables\")\n",
    "    \n",
    "    if not retained_cols:\n",
    "        print(f\"No significant predictors found for {station}. Skipping this station...\")\n",
    "        continue\n",
    "    \n",
    "    # Keep only YEAR, the rainfall column, and significant SST predictors\n",
    "    filtered_df = df_copy[[year_col, rainfall_col] + retained_cols]\n",
    "    filtered_sheet_dict[station] = filtered_df\n",
    "    \n",
    "    print(f\"Final data for {station}:\")\n",
    "    display(filtered_df.head())\n",
    "    print(f\"Shape: {filtered_df.shape} (rows, columns)\")\n",
    "    print(f\"Columns: {filtered_df.columns.tolist()}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Cell 4: Create Weighted SST Composite Index\n",
    "# This code creates weights based on correlation values and calculates a composite SST index\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dictionary to store dataframes with composite indices\n",
    "composite_df_dict = {}\n",
    "\n",
    "for station, filtered_df in filtered_sheet_dict.items():\n",
    "    print(f\"\\nProcessing station: {station}\")\n",
    "    \n",
    "    # Get the correlation dictionary for this station\n",
    "    station_corr = correlation_dict[station]\n",
    "    \n",
    "    # Identify SST columns (all columns except YEAR and the station rainfall)\n",
    "    year_col = \"YEAR\"\n",
    "    rainfall_col = station\n",
    "    sst_cols = [col for col in filtered_df.columns if col not in [year_col, rainfall_col]]\n",
    "    \n",
    "    if not sst_cols:\n",
    "        print(f\"No significant SST predictors for {station}. Skipping composite index creation.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Creating weights for significant SST columns: {sst_cols}\")\n",
    "    \n",
    "    # Calculate weights based on absolute correlation values\n",
    "    abs_corrs = {}\n",
    "    for col in sst_cols:\n",
    "        abs_corrs[col] = abs(station_corr[col]['correlation'])\n",
    "    \n",
    "    # Sum of absolute correlations\n",
    "    sum_abs_corr = sum(abs_corrs.values())\n",
    "    \n",
    "    # Calculate normalized weights\n",
    "    weights = {}\n",
    "    for col, abs_corr in abs_corrs.items():\n",
    "        weights[col] = abs_corr / sum_abs_corr\n",
    "    \n",
    "    # Display the weights\n",
    "    print(f\"Weights for {station}:\")\n",
    "    for col, weight in weights.items():\n",
    "        corr = station_corr[col]['correlation']\n",
    "        print(f\"  {col}: correlation = {corr:.4f}, weight = {weight:.4f}\")\n",
    "    \n",
    "    # Create a new dataframe with YEAR, rainfall, and a new composite index\n",
    "    df_composite = filtered_df[[year_col, rainfall_col]].copy()\n",
    "    \n",
    "    # Calculate the weighted composite SST index\n",
    "    df_composite['SST_COMPOSITE'] = 0\n",
    "    for col in sst_cols:\n",
    "        df_composite['SST_COMPOSITE'] += filtered_df[col] * weights[col]\n",
    "    \n",
    "    # Also keep the individual SST columns for reference\n",
    "    for col in sst_cols:\n",
    "        df_composite[col] = filtered_df[col]\n",
    "    \n",
    "    print(f\"Composite index created for {station}\")\n",
    "    print(\"First few rows of the dataframe with composite index:\")\n",
    "    print(df_composite.head())\n",
    "    \n",
    "    # Store the dataframe with composite index\n",
    "    composite_df_dict[station] = df_composite\n",
    "    \n",
    "    # Calculate correlation between composite index and rainfall\n",
    "    corr, p_value = stats.pearsonr(df_composite['SST_COMPOSITE'], df_composite[rainfall_col])\n",
    "    print(f\"Correlation between composite SST index and {station} rainfall: r={corr:.4f}, p={p_value:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nSummary of composite indices:\")\n",
    "for station, df in composite_df_dict.items():\n",
    "    print(f\"{station}: {df.shape[0]} years, {df.shape[1]-1} variables (including composite index)\")\n",
    "\n",
    "# The composite_df_dict now contains dataframes for each station with:\n",
    "# - YEAR\n",
    "# - Station rainfall\n",
    "# - SST_COMPOSITE (weighted index)\n",
    "# - Individual significant SST predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of known El Niño and La Niña years\n",
    "el_nino_years = [1961, 1997, 2013, 2019, 2023]\n",
    "la_nina_years = [1988, 1998, 2007, 2010, 2016, 2020]\n",
    "\n",
    "# Create model configurations\n",
    "model_configs = [\n",
    "    {\"Station\": station_name, \"Model\": model_type} \n",
    "    for station_name in composite_df_dict.keys() \n",
    "    for model_type in [\"Random Forest\", \"Gradient Boosting\", \"SVR\", \"Neural Network\"]\n",
    "]\n",
    "model_df = pd.DataFrame(model_configs)\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "# Function to add additional features\n",
    "def engineer_features(df):\n",
    "    df_new = df.copy()\n",
    "    if 'YEAR' in df_new.index.names:\n",
    "        df_new = df_new.reset_index()\n",
    "    df_new = df_new.sort_values('YEAR')\n",
    "    rainfall_col = df_new.columns[1]  # Station name\n",
    "    \n",
    "    # Simplified rolling statistics\n",
    "    if len(df) > 3:\n",
    "        df_new['rolling_mean_3yr'] = df_new[rainfall_col].rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        df_new['rolling_std_3yr'] = df_new[rainfall_col].rolling(window=3, min_periods=1).std().shift(1)\n",
    "        df_new['rolling_mean_3yr'] = df_new['rolling_mean_3yr'].fillna(df_new[rainfall_col].mean())\n",
    "        df_new['rolling_std_3yr'] = df_new['rolling_std_3yr'].fillna(df_new[rainfall_col].std())\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "for _, row in model_df.iterrows():\n",
    "    station = row[\"Station\"]\n",
    "    model_name = row[\"Model\"]\n",
    "\n",
    "    if station not in composite_df_dict:\n",
    "        print(f\"Station {station} not found in composite data. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df = composite_df_dict[station].copy()\n",
    "    print(f\"\\nWorking with {station} data from {df['YEAR'].min()} to {df['YEAR'].max()}\")\n",
    "    print(f\"Total data points: {len(df)}\")\n",
    "    \n",
    "    # Enhance El Niño and La Niña signals with higher weighting\n",
    "    df['is_el_nino'] = df['YEAR'].apply(lambda x: 1 if x in el_nino_years else 0)\n",
    "    df['is_la_nina'] = df['YEAR'].apply(lambda x: 1 if x in la_nina_years else 0)\n",
    "    df['recent_strong_el_nino'] = df['YEAR'].apply(lambda x: 2 if x in [2019, 2023] else (1 if x == 2013 else 0))\n",
    "    df['recent_strong_la_nina'] = df['YEAR'].apply(lambda x: 2 if x in [2016, 2020] else (1 if x == 2010 else 0))\n",
    "    df['el_nino_weight'] = df['is_el_nino'] * (1 + df['recent_strong_el_nino'] * 1.0)\n",
    "    df['la_nina_weight'] = df['is_la_nina'] * (1 + df['recent_strong_la_nina'] * 1.0)\n",
    "    \n",
    "    # Add engineered features\n",
    "    df = engineer_features(df)\n",
    "    df = df.sort_values('YEAR')\n",
    "    \n",
    "    # Define features and target\n",
    "    target_col = station  # Station name is the target (rainfall)\n",
    "    years = df['YEAR'].values\n",
    "    \n",
    "    # Use all columns except YEAR and the target (rainfall) column\n",
    "    feature_columns = [col for col in df.columns if col != 'YEAR' and col != target_col]\n",
    "    print(f\"Features used: {', '.join(feature_columns)}\")\n",
    "    \n",
    "    X = df[feature_columns]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    if X.shape[1] == 0:\n",
    "        print(f\"No features available for {station}. Skipping model creation.\")\n",
    "        continue\n",
    "    \n",
    "    # Select top features using Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "    top_features = importances.nlargest(6).index.tolist()\n",
    "    \n",
    "    # Ensure SST_COMPOSITE is always included in top features\n",
    "    if 'SST_COMPOSITE' not in top_features:\n",
    "        top_features = ['SST_COMPOSITE'] + top_features[:5]\n",
    "    \n",
    "    # Make sure El Niño and La Niña indicators are considered\n",
    "    enso_features = ['el_nino_weight', 'la_nina_weight']\n",
    "    for feature in enso_features:\n",
    "        if feature not in top_features and feature in X.columns:\n",
    "            if len(top_features) >= 6:\n",
    "                top_features = top_features[:5] + [feature]\n",
    "            else:\n",
    "                top_features.append(feature)\n",
    "        \n",
    "    print(f\"Top features for {station}: {top_features}\")\n",
    "    X = X[top_features]\n",
    "    \n",
    "    if len(X) < 5:\n",
    "        print(f\"Not enough data points for {station}. Skipping model creation.\")\n",
    "        continue\n",
    "\n",
    "    # Split data into training, validation, and test sets\n",
    "    train_size = 0.7\n",
    "    val_size = 0.15\n",
    "    test_size = 0.15\n",
    "    train_end = int(len(X) * train_size)\n",
    "    val_end = train_end + int(len(X) * val_size)\n",
    "\n",
    "    print(f\"Training samples: {train_end}\")\n",
    "    print(f\"Validation samples: {val_end - train_end}\")\n",
    "    print(f\"Testing samples: {len(X) - val_end}\")\n",
    "\n",
    "    X_train = X.iloc[:train_end]\n",
    "    y_train = y.iloc[:train_end]\n",
    "    years_train = years[:train_end]\n",
    "    X_val = X.iloc[train_end:val_end]\n",
    "    y_val = y.iloc[train_end:val_end]\n",
    "    years_val = years[train_end:val_end]\n",
    "    X_test = X.iloc[val_end:]\n",
    "    y_test = y.iloc[val_end:]\n",
    "    years_test = years[val_end:]\n",
    "\n",
    "    print(f\"Training years: {min(years_train)} to {max(years_train)}\")\n",
    "    print(f\"Validation years: {min(years_val)} to {max(years_val)}\")\n",
    "    print(f\"Testing years: {min(years_test)} to {max(years_test)}\")\n",
    "    \n",
    "    # Configure and train the model based on the model type\n",
    "    if model_name == \"Random Forest\":\n",
    "        model = Pipeline([\n",
    "            ('scaler', RobustScaler()),\n",
    "            ('model', RandomForestRegressor(\n",
    "                n_estimators=100, \n",
    "                max_depth=5,\n",
    "                min_samples_leaf=3,\n",
    "                min_samples_split=5,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    elif model_name == \"Gradient Boosting\":\n",
    "        model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', GradientBoostingRegressor(\n",
    "                loss='huber', \n",
    "                n_estimators=100,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=3,\n",
    "                min_samples_leaf=3,\n",
    "                subsample=0.8,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    elif model_name == \"SVR\":\n",
    "        model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', SVR(\n",
    "                kernel='rbf',\n",
    "                C=1.0,\n",
    "                epsilon=0.1,\n",
    "                gamma='scale'\n",
    "            ))\n",
    "        ])\n",
    "    elif model_name == \"Neural Network\":\n",
    "        model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', MLPRegressor(\n",
    "                hidden_layer_sizes=(50, 25),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                alpha=0.01,\n",
    "                max_iter=2000,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.1,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    else:\n",
    "        print(f\"Unknown model type: {model_name}. Skipping...\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Cross-validation on training data\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_r2_scores = []\n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            model.fit(X_cv_train, y_cv_train)\n",
    "            y_cv_pred = model.predict(X_cv_val)\n",
    "            cv_r2 = r2_score(y_cv_val, y_cv_pred)\n",
    "            cv_r2_scores.append(cv_r2)\n",
    "        print(f\"Cross-validation R² scores for {station} - {model_name}: {cv_r2_scores}\")\n",
    "        print(f\"Average CV R²: {np.mean(cv_r2_scores):.3f}\")\n",
    "\n",
    "        # Train on full training set\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Metrics for all sets\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        val_r2 = r2_score(y_val, y_val_pred)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        \n",
    "        # Calculate special metrics for drought events (values below zero)\n",
    "        below_zero_indices = np.where(y_test < 0)[0]\n",
    "        if len(below_zero_indices) > 0:\n",
    "            below_zero_rmse = np.sqrt(mean_squared_error(y_test.iloc[below_zero_indices], y_test_pred[below_zero_indices]))\n",
    "            print(f\"  RMSE for values below zero: {below_zero_rmse:.3f}\")\n",
    "            print(f\"  Number of below-zero actual values in test set: {len(below_zero_indices)}\")\n",
    "        else:\n",
    "            below_zero_rmse = None\n",
    "            print(\"  No below-zero values in test set to evaluate\")\n",
    "        \n",
    "        # Store results\n",
    "        results_dict.setdefault(station, {})[model_name] = {\n",
    "            'rmse': test_rmse,\n",
    "            'r2': test_r2,\n",
    "            'mae': test_mae,\n",
    "            'below_zero_rmse': below_zero_rmse,\n",
    "            'predictions': y_test_pred,\n",
    "            'actuals': y_test.values,\n",
    "            'years': years_test\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nResults for {station} using {model_name}:\")\n",
    "        print(f\"  Train R² = {train_r2:.3f}, RMSE = {train_rmse:.3f}\")\n",
    "        print(f\"  Validation R² = {val_r2:.3f}, RMSE = {val_rmse:.3f}\")\n",
    "        print(f\"  Test R² = {test_r2:.3f}, RMSE = {test_rmse:.3f}, MAE = {test_mae:.3f}\")\n",
    "        \n",
    "        # Generate plots\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(years_test, y_test.values, label=\"Observed\", marker='o', color='blue')\n",
    "        plt.plot(years_test, y_test_pred, label=\"Predicted\", linestyle='--', marker='x', color='red')\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.title(f\"{station} - {model_name}: Observed vs Predicted (Test Set)\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(f\"{target_col}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(years_test)\n",
    "        if len(years_test) > 6:\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        errors = y_test.values - y_test_pred\n",
    "        plt.bar(years_test, errors, color='purple', alpha=0.6)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.title(\"Prediction Errors (Actual - Predicted)\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.xticks(years_test)\n",
    "        if len(years_test) > 6:\n",
    "            plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Scatter plot of actual vs predicted values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test.values, y_test_pred, alpha=0.6)\n",
    "        min_val = min(min(y_test.values), min(y_test_pred))\n",
    "        max_val = max(max(y_test.values), max(y_test_pred))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "        plt.axvline(x=0, color='red', linestyle='--', alpha=0.3)\n",
    "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "        plt.title(f\"{station} - {model_name}: Actual vs Predicted Values\")\n",
    "        plt.xlabel(\"Actual Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name} for {station}: {str(e)}\")\n",
    "\n",
    "# Create summary table of results\n",
    "summary_rows = []\n",
    "for station in results_dict:\n",
    "    station_models = results_dict[station]\n",
    "    if not station_models:\n",
    "        continue\n",
    "        \n",
    "    best_model_name = max(station_models.keys(), key=lambda k: station_models[k]['r2'])\n",
    "    best_model = station_models[best_model_name]\n",
    "    \n",
    "    # Get the features used for this station\n",
    "    station_df = composite_df_dict[station]\n",
    "    features_used = top_features\n",
    "    \n",
    "    summary_rows.append({\n",
    "        'Station': station,\n",
    "        'Best Model': best_model_name,\n",
    "        'R2': best_model['r2'],\n",
    "        'RMSE': best_model['rmse'],\n",
    "        'MAE': best_model['mae'],\n",
    "        'Below Zero RMSE': best_model.get('below_zero_rmse', 'N/A'),\n",
    "        'Features Used': ', '.join(features_used)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary of Best Models for Each Station\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add a feature importance plot for the best models\n",
    "print(\"\\nFeature Importance for Best Models:\")\n",
    "for station in results_dict:\n",
    "    station_models = results_dict[station]\n",
    "    if not station_models:\n",
    "        continue\n",
    "        \n",
    "    best_model_name = max(station_models.keys(), key=lambda k: station_models[k]['r2'])\n",
    "    \n",
    "    # Skip models that don't support feature importance\n",
    "    if best_model_name in [\"SVR\", \"Neural Network\"]:\n",
    "        print(f\"  {station} - {best_model_name}: Feature importance not available for this model type\")\n",
    "        continue\n",
    "    \n",
    "    # Get the pipeline model\n",
    "    model_dict = model_df[(model_df[\"Station\"] == station) & (model_df[\"Model\"] == best_model_name)]\n",
    "    if model_dict.empty:\n",
    "        continue\n",
    "    \n",
    "    # Extract the trained model\n",
    "    for _, row in model_df.iterrows():\n",
    "        if row[\"Station\"] == station and row[\"Model\"] == best_model_name:\n",
    "            try:\n",
    "                # Get feature importance\n",
    "                station_df = composite_df_dict[station]\n",
    "                feature_columns = [col for col in station_df.columns if col != 'YEAR' and col != station]\n",
    "                \n",
    "                # Select top features using Random Forest\n",
    "                rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                X = station_df[feature_columns]\n",
    "                y = station_df[station]\n",
    "                rf.fit(X, y)\n",
    "                \n",
    "                # Plot feature importance\n",
    "                importances = pd.Series(rf.feature_importances_, index=feature_columns)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                importances.sort_values().plot(kind='barh')\n",
    "                plt.title(f'Feature Importance for {station} using Random Forest')\n",
    "                plt.xlabel('Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"  {station} - Feature Importance:\")\n",
    "                for feature, importance in importances.sort_values(ascending=False).items():\n",
    "                    print(f\"    {feature}: {importance:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting feature importance for {station}: {str(e)}\")\n",
    "                \n",
    "print(\"\\nDrought Forecasting Model Training Complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca4c8e-2d3c-4da9-9110-3ad4667478db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL SUMMARY\n",
    "\n",
    "best_models_summary = pd.DataFrame(columns=[\n",
    "    'Station', 'Best Model', 'R2', 'RMSE', 'MAE', 'Features Used'\n",
    "])\n",
    "\n",
    "forecasting_dfs = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL SELECTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for station, models_dict in results_dict.items():\n",
    "    print(f\"\\nAnalyzing models for station: {station}\")\n",
    "    \n",
    "    best_model_name = None\n",
    "    best_r2 = float('-inf')\n",
    "    best_metrics = {}\n",
    "\n",
    "    # Find the model with the highest R² value from the results_dict (from cell 4)\n",
    "    for model_name, metrics in models_dict.items():\n",
    "        r2 = metrics['r2']\n",
    "        print(f\"  {model_name}: R² = {r2:.4f}, RMSE = {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model_name = model_name\n",
    "            best_metrics = metrics\n",
    "    \n",
    "    if best_model_name:\n",
    "        try:\n",
    "            print(f\"\\n✅ Best model for {station}: {best_model_name}\")\n",
    "            print(f\"  R² = {best_r2:.4f}\")\n",
    "            print(f\"  RMSE = {best_metrics['rmse']:.4f}\")\n",
    "            print(f\"  MAE = {best_metrics['mae']:.4f}\")\n",
    "            \n",
    "            # Get the original dataframe for this station\n",
    "            station_df = filtered_sheet_dict[station].copy()\n",
    "            \n",
    "            # Get the features used for this station - use exactly the same features from cell 4\n",
    "            features_used = [col for col in station_df.columns if col not in ['YEAR', station]]\n",
    "            features_str = ', '.join(features_used)\n",
    "            \n",
    "            # Add El Niño indicator for future forecasting\n",
    "            el_nino_years = [1997, 2013, 2019, 2023]\n",
    "            station_df['is_el_nino'] = station_df['YEAR'].apply(lambda x: 1 if int(x) in el_nino_years else 0)\n",
    "            \n",
    "            best_models_summary = pd.concat([\n",
    "                best_models_summary,\n",
    "                pd.DataFrame({\n",
    "                    'Station': [station],\n",
    "                    'Best Model': [best_model_name],\n",
    "                    'R2': [best_r2],\n",
    "                    'RMSE': [best_metrics['rmse']],\n",
    "                    'MAE': [best_metrics['mae']],\n",
    "                    'Features Used': [features_str]\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # Store the original dataframe with the best model predictions for forecasting\n",
    "            forecasting_dfs[station] = {\n",
    "                'df': station_df,\n",
    "                'model_name': best_model_name,\n",
    "                'features': features_used,\n",
    "                'target': station,\n",
    "                'years': best_metrics['years'],\n",
    "                'actuals': best_metrics['actuals'],\n",
    "                'predictions': best_metrics['predictions'],\n",
    "                'r2': best_r2,\n",
    "                'rmse': best_metrics['rmse'],\n",
    "                'mae': best_metrics['mae']\n",
    "            }\n",
    "\n",
    "            \n",
    "            years_test = best_metrics['years']\n",
    "            y_test = best_metrics['actuals']\n",
    "            y_pred = best_metrics['predictions']\n",
    "            target_col = station\n",
    "\n",
    "            print(\"Years in test set:\", years_test)\n",
    "\n",
    "            if len(set(years_test)) == 1:\n",
    "                print(\"WARNING: All test data is from the same year!\")\n",
    "\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            if len(set(years_test)) == 1:\n",
    "                x_vals = np.arange(len(y_test))\n",
    "                plt.plot(x_vals, y_test, label=\"Observed\", marker='o')\n",
    "                plt.plot(x_vals, y_pred, label=\"Predicted\", linestyle='--', marker='x')\n",
    "                plt.xticks(x_vals, [years_test[0]] * len(y_test))\n",
    "            else:\n",
    "                plt.plot(years_test, y_test, label=\"Observed\", marker='o')\n",
    "                plt.plot(years_test, y_pred, label=\"Predicted\", linestyle='--', marker='x')\n",
    "\n",
    "            plt.title(f\"{station} - {best_model_name}: Observed vs Predicted (Test Set)\")\n",
    "            plt.xlabel(\"Year\")\n",
    "            plt.ylabel(f\"{target_col}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling best model for {station}: {str(e)}\")\n",
    "            \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODELS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(best_models_summary)\n",
    "\n",
    "print(\"\\nPrepared forecasting dataframes for these stations:\")\n",
    "for station in forecasting_dfs.keys():\n",
    "    print(f\"- {station}\")\n",
    "\n",
    "print(\"\\nForecasting-ready dataframes and best models are stored in 'forecasting_dfs'\")\n",
    "print(\"You can use these in cell 6 for drought prediction and forecasting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e72a59-dccd-40e5-acc5-cf375047a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HITS AND MISSES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "\n",
    "def calculate_dynamic_drought_threshold(\n",
    "    data, \n",
    "    percentile=25,  \n",
    "    mad_multiplier=5,  \n",
    "    min_threshold=None  \n",
    "):\n",
    "    \"\"\"\n",
    "    More flexible drought threshold calculation\n",
    "    \"\"\"\n",
    "    # Remove extreme outliers using Median Absolute Deviation (MAD)\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    \n",
    "    # Create a mask to remove outliers\n",
    "    outlier_mask = np.abs(data - median) <= (mad_multiplier * mad)\n",
    "    cleaned_data = data[outlier_mask]\n",
    "    \n",
    "    # Calculate threshold based on percentile of cleaned data\n",
    "    threshold = np.percentile(cleaned_data, percentile)\n",
    "    \n",
    "    # Optional minimum threshold check\n",
    "    if min_threshold is not None:\n",
    "        threshold = max(threshold, min_threshold)\n",
    "    \n",
    "    return threshold\n",
    "\n",
    "def comprehensive_drought_threshold_analysis(results_dict):\n",
    "    \"\"\"\n",
    "    Explore multiple drought threshold configurations\n",
    "    \"\"\"\n",
    "    # Threshold exploration parameters\n",
    "    percentile_options = [10, 20, 25, 30, 40]\n",
    "    mad_multiplier_options = [3, 4, 5, 6]\n",
    "    \n",
    "    # Comprehensive results storage\n",
    "    comprehensive_results = {}\n",
    "    \n",
    "    # Visualize threshold exploration\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    for station, models in results_dict.items():\n",
    "        # Collect all actual values\n",
    "        all_actuals = np.concatenate([models[model]['actuals'] for model in models])\n",
    "        \n",
    "        # Store results for this station\n",
    "        station_results = []\n",
    "        \n",
    "        # Explore different threshold configurations\n",
    "        for percentile, mad_multiplier in product(percentile_options, mad_multiplier_options):\n",
    "            # Calculate dynamic threshold\n",
    "            drought_threshold = calculate_dynamic_drought_threshold(\n",
    "                all_actuals, \n",
    "                percentile=percentile, \n",
    "                mad_multiplier=mad_multiplier\n",
    "            )\n",
    "            \n",
    "            # Evaluate across all models\n",
    "            station_model_performances = []\n",
    "            for model_name, model_data in models.items():\n",
    "                # Threshold evaluation metrics\n",
    "                actuals = model_data['actuals']\n",
    "                predictions = model_data['predictions']\n",
    "                years = model_data['years']\n",
    "                \n",
    "                # Identify drought events\n",
    "                actual_droughts = actuals <= drought_threshold\n",
    "                predicted_droughts = predictions <= drought_threshold\n",
    "                \n",
    "                # Confusion matrix metrics\n",
    "                true_positives = np.sum((actual_droughts) & (predicted_droughts))\n",
    "                false_positives = np.sum((~actual_droughts) & (predicted_droughts))\n",
    "                true_negatives = np.sum((~actual_droughts) & (~predicted_droughts))\n",
    "                false_negatives = np.sum((actual_droughts) & (~predicted_droughts))\n",
    "                \n",
    "                # Performance calculations\n",
    "                try:\n",
    "                    drought_detection_rate = true_positives / np.sum(actual_droughts) if np.sum(actual_droughts) > 0 else 0\n",
    "                    false_alarm_rate = false_positives / np.sum(~actual_droughts) if np.sum(~actual_droughts) > 0 else 0\n",
    "                    f1_score = (2 * true_positives) / (2 * true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) > 0 else 0\n",
    "                except ZeroDivisionError:\n",
    "                    drought_detection_rate = false_alarm_rate = f1_score = 0\n",
    "                \n",
    "                station_model_performances.append({\n",
    "                    'model': model_name,\n",
    "                    'true_positives': true_positives,\n",
    "                    'false_positives': false_positives,\n",
    "                    'true_negatives': true_negatives,\n",
    "                    'false_negatives': false_negatives,\n",
    "                    'drought_detection_rate': drought_detection_rate,\n",
    "                    'false_alarm_rate': false_alarm_rate,\n",
    "                    'f1_score': f1_score\n",
    "                })\n",
    "            \n",
    "            # Find best performing model for this threshold configuration\n",
    "            best_model = max(station_model_performances, key=lambda x: x['f1_score'])\n",
    "            \n",
    "            # Store results\n",
    "            result_entry = {\n",
    "                'percentile': percentile,\n",
    "                'mad_multiplier': mad_multiplier,\n",
    "                'drought_threshold': drought_threshold,\n",
    "                'best_model': best_model['model'],\n",
    "                **best_model\n",
    "            }\n",
    "            station_results.append(result_entry)\n",
    "        \n",
    "        # Store results for this station\n",
    "        comprehensive_results[station] = station_results\n",
    "        \n",
    "        # Visualize F1 Scores for this station\n",
    "        plt.subplot(3, 3, list(results_dict.keys()).index(station) + 1)\n",
    "        station_df = pd.DataFrame(station_results)\n",
    "        \n",
    "        # Pivot table for heatmap\n",
    "        f1_pivot = station_df.pivot(\n",
    "            index='percentile', \n",
    "            columns='mad_multiplier', \n",
    "            values='f1_score'\n",
    "        )\n",
    "        \n",
    "        sns.heatmap(f1_pivot, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "        plt.title(f'{station} - Threshold F1 Score Heatmap')\n",
    "        plt.xlabel('MAD Multiplier')\n",
    "        plt.ylabel('Percentile')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create comprehensive summary\n",
    "    summary_results = []\n",
    "    for station, results in comprehensive_results.items():\n",
    "        # Find overall best configuration\n",
    "        best_config = max(results, key=lambda x: x['f1_score'])\n",
    "        summary_results.append({\n",
    "            'Station': station,\n",
    "            'Best Percentile': best_config['percentile'],\n",
    "            'Best MAD Multiplier': best_config['mad_multiplier'],\n",
    "            'Drought Threshold': best_config['drought_threshold'],\n",
    "            'Best Model': best_config['best_model'],\n",
    "            'Best F1 Score': best_config['f1_score'],\n",
    "            'Drought Detection Rate': best_config['drought_detection_rate'],\n",
    "            'False Alarm Rate': best_config['false_alarm_rate']\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and display\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    print(\"\\nComprehensive Drought Threshold Analysis Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return comprehensive_results, summary_df\n",
    "\n",
    "# Run the analysis\n",
    "try:\n",
    "    # Explore and visualize drought threshold configurations\n",
    "    comprehensive_results, summary_df = comprehensive_drought_threshold_analysis(results_dict)\n",
    "    \n",
    "    # Optional: Save results to CSV\n",
    "    summary_df.to_csv('drought_threshold_exploration.csv', index=False)\n",
    "    print(\"\\nDetailed results saved to drought_threshold_exploration.csv\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nResults dictionary not found. Ensure the model training script has been run first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b68159-6c70-4001-8c61-c850c0eddb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUTURE PREDICTION\n",
    "\n",
    "class StationDroughtAnalyzer:\n",
    "    def __init__(self, station_name, rainfall_data, years):\n",
    "        \"\"\"\n",
    "        Initialize drought analyzer for a specific station\n",
    "        \n",
    "        :param station_name: Name of the weather station\n",
    "        :param rainfall_data: Numpy array or list of rainfall values\n",
    "        :param years: Corresponding years for rainfall data\n",
    "        \"\"\"\n",
    "        self.station_name = station_name\n",
    "        self.rainfall_data = np.array(rainfall_data)\n",
    "        self.years = np.array(years)\n",
    "        \n",
    "        # Compute station-specific drought thresholds\n",
    "        self.compute_drought_thresholds()\n",
    "    \n",
    "    def compute_drought_thresholds(self):\n",
    "        \"\"\"\n",
    "        Compute multiple drought thresholds based on statistical analysis\n",
    "        \"\"\"\n",
    "        # Calculate various statistical measures\n",
    "        self.mean_rainfall = np.mean(self.rainfall_data)\n",
    "        self.median_rainfall = np.median(self.rainfall_data)\n",
    "        self.std_rainfall = np.std(self.rainfall_data)\n",
    "        \n",
    "        # Define multiple drought severity levels\n",
    "        self.thresholds = {\n",
    "            'moderate': self.mean_rainfall - 0.5 * self.std_rainfall,  # 1st level: below mean by 0.5 standard deviations\n",
    "            'severe': self.mean_rainfall - 1.0 * self.std_rainfall,    # 2nd level: below mean by 1 standard deviation\n",
    "            'extreme': self.mean_rainfall - 1.5 * self.std_rainfall    # 3rd level: below mean by 1.5 standard deviations\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{self.station_name} Drought Thresholds:\")\n",
    "        for severity, threshold in self.thresholds.items():\n",
    "            print(f\"  {severity.capitalize()} Drought: {threshold:.2f}\")\n",
    "    \n",
    "    def analyze_historical_droughts(self, drought_level='moderate'):\n",
    "        \"\"\"\n",
    "        Analyze historical droughts for a given severity level\n",
    "        \n",
    "        :param drought_level: Severity of drought to analyze ('moderate', 'severe', 'extreme')\n",
    "        :return: Dictionary of drought analysis results\n",
    "        \"\"\"\n",
    "        # Validate drought level\n",
    "        if drought_level not in self.thresholds:\n",
    "            raise ValueError(f\"Invalid drought level. Choose from {list(self.thresholds.keys())}\")\n",
    "        \n",
    "        # Get the specific threshold\n",
    "        drought_threshold = self.thresholds[drought_level]\n",
    "        \n",
    "        # Identify drought years\n",
    "        drought_mask = self.rainfall_data <= drought_threshold\n",
    "        drought_years = self.years[drought_mask]\n",
    "        drought_values = self.rainfall_data[drought_mask]\n",
    "        \n",
    "        # Compute drought frequency\n",
    "        drought_frequency = len(drought_years) / len(self.years) * 100\n",
    "        \n",
    "        # Compute intervals between droughts if more than one drought occurred\n",
    "        drought_intervals = None\n",
    "        avg_interval = None\n",
    "        if len(drought_years) > 1:\n",
    "            drought_intervals = np.diff(drought_years)\n",
    "            avg_interval = np.mean(drought_intervals)\n",
    "        \n",
    "        # Prepare results dictionary\n",
    "        results = {\n",
    "            'drought_threshold': drought_threshold,\n",
    "            'drought_years': drought_years.tolist(),\n",
    "            'drought_values': drought_values.tolist(),\n",
    "            'drought_frequency': drought_frequency,\n",
    "            'avg_drought_interval': avg_interval\n",
    "        }\n",
    "        \n",
    "        # Print analysis results\n",
    "        print(f\"\\n{self.station_name} {drought_level.capitalize()} Drought Analysis:\")\n",
    "        print(f\"  Drought Threshold: {drought_threshold:.2f}\")\n",
    "        print(f\"  Drought Years: {drought_years}\")\n",
    "        print(f\"  Drought Frequency: {drought_frequency:.1f}%\")\n",
    "        if avg_interval is not None:\n",
    "            print(f\"  Average Drought Interval: {avg_interval:.1f} years\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def forecast_droughts(self, drought_level='moderate', forecast_period=10):\n",
    "        \"\"\"\n",
    "        Forecast future droughts using statistical projection\n",
    "        \n",
    "        :param drought_level: Severity of drought to predict\n",
    "        :param forecast_period: Number of years to forecast\n",
    "        :return: Dictionary of forecast results\n",
    "        \"\"\"\n",
    "        # Get the specific threshold for the drought level\n",
    "        drought_threshold = self.thresholds[drought_level]\n",
    "        \n",
    "        try:\n",
    "            # Basic linear regression forecast\n",
    "            X = np.arange(len(self.years)).reshape(-1, 1)\n",
    "            y = self.rainfall_data\n",
    "            \n",
    "            # Perform linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(X.flatten(), y)\n",
    "            \n",
    "            # Generate forecast years\n",
    "            last_year = self.years[-1]\n",
    "            forecast_years = np.arange(last_year + 1, last_year + forecast_period + 1)\n",
    "            \n",
    "            # Project future values based on trend\n",
    "            forecast = intercept + slope * np.arange(len(self.years), len(self.years) + forecast_period)\n",
    "            \n",
    "            # Add some randomness to make it more realistic\n",
    "            forecast += np.random.normal(0, std_err, forecast_period)\n",
    "            \n",
    "            # Identify drought years in forecast\n",
    "            forecast_drought_mask = forecast <= drought_threshold\n",
    "            predicted_drought_years = forecast_years[forecast_drought_mask]\n",
    "            predicted_drought_values = forecast[forecast_drought_mask]\n",
    "            \n",
    "            # Prepare forecast results\n",
    "            forecast_results = {\n",
    "                'forecast_years': forecast_years.tolist(),\n",
    "                'forecast_values': forecast.tolist(),\n",
    "                'predicted_drought_years': predicted_drought_years.tolist(),\n",
    "                'predicted_drought_values': predicted_drought_values.tolist()\n",
    "            }\n",
    "            \n",
    "            # Print forecast results\n",
    "            print(f\"\\n{self.station_name} {drought_level.capitalize()} Drought Forecast:\")\n",
    "            print(\"Year | Forecasted Rainfall | Drought Status\")\n",
    "            print(\"-\" * 45)\n",
    "            for year, value in zip(forecast_years, forecast):\n",
    "                status = \"DROUGHT\" if value <= drought_threshold else \"Normal\"\n",
    "                print(f\"{year} | {value:7.2f}        | {status}\")\n",
    "            \n",
    "            return forecast_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in forecast for {self.station_name}: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "            return None\n",
    "    \n",
    "    def visualize_drought_analysis(self, historical_analysis, forecast_results=None, drought_level='moderate'):\n",
    "        \"\"\"\n",
    "        Create a comprehensive visualization of drought analysis\n",
    "        \n",
    "        :param historical_analysis: Results from historical drought analysis\n",
    "        :param forecast_results: Results from drought forecast (optional)\n",
    "        :param drought_level: Severity of drought being analyzed\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot full rainfall data\n",
    "        plt.plot(self.years, self.rainfall_data, 'b-', label='Historical Rainfall')\n",
    "        \n",
    "        # Highlight historical drought years\n",
    "        drought_mask = np.array(self.rainfall_data) <= historical_analysis['drought_threshold']\n",
    "        plt.scatter(\n",
    "            self.years[drought_mask], \n",
    "            np.array(self.rainfall_data)[drought_mask], \n",
    "            color='red', s=50, label='Historical Droughts'\n",
    "        )\n",
    "        \n",
    "        # Add drought threshold line\n",
    "        plt.axhline(\n",
    "            y=historical_analysis['drought_threshold'], \n",
    "            color='r', \n",
    "            linestyle='--', \n",
    "            label=f'{drought_level.capitalize()} Drought Threshold'\n",
    "        )\n",
    "        \n",
    "        # Plot forecast if available\n",
    "        if forecast_results:\n",
    "            forecast_years = forecast_results['forecast_years']\n",
    "            forecast_values = forecast_results['forecast_values']\n",
    "            plt.plot(forecast_years, forecast_values, 'g--', label='Statistical Forecast')\n",
    "            \n",
    "            # Highlight predicted drought years\n",
    "            if forecast_results['predicted_drought_years']:\n",
    "                plt.scatter(\n",
    "                    forecast_results['predicted_drought_years'], \n",
    "                    forecast_results['predicted_drought_values'], \n",
    "                    color='orange', marker='x', s=50, \n",
    "                    label='Predicted Droughts'\n",
    "                )\n",
    "        \n",
    "        plt.title(f\"{self.station_name} - {drought_level.capitalize()} Drought Analysis\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Rainfall\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def perform_comprehensive_drought_analysis(results_dict):\n",
    "    \"\"\"\n",
    "    Perform comprehensive drought analysis for all stations\n",
    "    \n",
    "    :param results_dict: Dictionary containing station results from previous analysis\n",
    "    :return: Comprehensive drought analysis results\n",
    "    \"\"\"\n",
    "    comprehensive_drought_analysis = {}\n",
    "    \n",
    "    for station in results_dict.keys():\n",
    "        print(f\"\\n{'='*70}\\n{station} COMPREHENSIVE DROUGHT ANALYSIS\\n{'='*70}\")\n",
    "        \n",
    "        # Get the best model results for this station\n",
    "        best_model_name = max(results_dict[station].keys(), key=lambda k: results_dict[station][k]['r2'])\n",
    "        station_results = results_dict[station][best_model_name]\n",
    "        \n",
    "        # Extract years and rainfall data\n",
    "        years = station_results['years']\n",
    "        rainfall = station_results['actuals']\n",
    "        \n",
    "        # Create station-specific drought analyzer\n",
    "        analyzer = StationDroughtAnalyzer(station, rainfall, years)\n",
    "        \n",
    "        # Analyze droughts at different severity levels\n",
    "        drought_analysis = {}\n",
    "        for severity in ['moderate', 'severe', 'extreme']:\n",
    "            # Historical drought analysis\n",
    "            historical_analysis = analyzer.analyze_historical_droughts(drought_level=severity)\n",
    "            \n",
    "            # Forecast droughts\n",
    "            forecast_results = analyzer.forecast_droughts(drought_level=severity)\n",
    "            \n",
    "            # Visualize results\n",
    "            analyzer.visualize_drought_analysis(historical_analysis, forecast_results, drought_level=severity)\n",
    "            \n",
    "            # Store results\n",
    "            drought_analysis[severity] = {\n",
    "                'historical': historical_analysis,\n",
    "                'forecast': forecast_results\n",
    "            }\n",
    "        \n",
    "        # Store comprehensive analysis for this station\n",
    "        comprehensive_drought_analysis[station] = drought_analysis\n",
    "    \n",
    "    return comprehensive_drought_analysis\n",
    "\n",
    "# Run the comprehensive drought analysis\n",
    "comprehensive_results = perform_comprehensive_drought_analysis(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a28a1-caba-4aa9-894e-9a6e64a37651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7d7ea-a903-4fef-8f86-2e8464db0505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
