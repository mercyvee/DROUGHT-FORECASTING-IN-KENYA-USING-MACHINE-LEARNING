{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91103136",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Admin/Desktop/DROUGHT-FORECASTING-IN-KENYA-USING-MACHINE-LEARNING/MLFile2.xls'  # Replace with actual path\n",
    "sheet_names = pd.ExcelFile(file_path).sheet_names\n",
    "sheet_dict = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}\n",
    "print (sheet_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb45602-ab81-4b25-ab15-909aeb7bbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sheet_dict = {}\n",
    "correlation_dict = {} \n",
    "\n",
    "for station, df in sheet_dict.items():\n",
    "    print(f\"\\nProcessing station: {station}\")\n",
    "    print(f\"Original columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy.dropna(inplace=True)\n",
    "    \n",
    "\n",
    "    if df_copy.empty:\n",
    "        print(f\"No data left for {station} after dropping missing values. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    year_col = \"YEAR\"\n",
    "    rainfall_col = station  \n",
    "    \n",
    "    print(f\"Year column: {year_col}\")\n",
    "    print(f\"Rainfall column: {rainfall_col}\")\n",
    "    \n",
    "    sst_cols = [col for col in df_copy.columns if col not in [year_col, rainfall_col]]\n",
    "    print(f\"SST columns: {sst_cols}\")\n",
    "    \n",
    "    print(f\"\\nCorrelation results for {station}:\")\n",
    "    \n",
    "    retained_cols = []\n",
    "    station_correlations = {}  # Store correlations for this station\n",
    "    \n",
    "    for col in sst_cols:\n",
    "        try:\n",
    "            corr, p_value = stats.pearsonr(df_copy[col], df_copy[rainfall_col])\n",
    "            print(f\"{col}: r={corr:.2f}, p={p_value:.3f}\")\n",
    "            station_correlations[col] = {'correlation': corr, 'p_value': p_value}  # Save correlation stats\n",
    "            \n",
    "            if p_value <= 0.05:\n",
    "                retained_cols.append(col)\n",
    "            else:\n",
    "                print(f\"Dropped {col} from {station} (p={p_value:.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column {col}: {str(e)}\")\n",
    "    \n",
    "    correlation_dict[station] = station_correlations\n",
    "    \n",
    "    print(f\"Retained {len(retained_cols)} predictors out of {len(sst_cols)} original variables\")\n",
    "    \n",
    "    if not retained_cols:\n",
    "        print(f\"No significant predictors found for {station}. Skipping this station...\")\n",
    "        continue\n",
    "    \n",
    "    # Keep only YEAR, the rainfall column, and significant SST predictors\n",
    "    filtered_df = df_copy[[year_col, rainfall_col] + retained_cols]\n",
    "    filtered_sheet_dict[station] = filtered_df\n",
    "    \n",
    "    print(f\"Final data for {station}:\")\n",
    "    display(filtered_df.head())\n",
    "    print(f\"Shape: {filtered_df.shape} (rows, columns)\")\n",
    "    print(f\"Columns: {filtered_df.columns.tolist()}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# List of known El Niño years\n",
    "el_nino_years = [1997, 2013, 2019, 2023]\n",
    "\n",
    "# Create model combinations\n",
    "model_configs = [\n",
    "    {\"Station\": station_name, \"Model\": model_type} \n",
    "    for station_name in filtered_sheet_dict.keys() \n",
    "    for model_type in [\"Random Forest\", \"Gradient Boosting\", \"SVR\", \"Neural Network\"]\n",
    "]\n",
    "model_df = pd.DataFrame(model_configs)\n",
    "\n",
    "# Container for results\n",
    "results_dict = {}\n",
    "\n",
    "# Function to add additional features that help with extreme values\n",
    "def engineer_features(df):\n",
    "    # Copy to avoid modifying original\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Calculate rolling averages if enough data is available\n",
    "    if len(df) > 3:\n",
    "        # If YEAR is in index, reset it\n",
    "        if 'YEAR' in df_new.index.names:\n",
    "            df_new = df_new.reset_index()\n",
    "        \n",
    "        # Sort by year for proper rolling calculations\n",
    "        df_new = df_new.sort_values('YEAR')\n",
    "        \n",
    "        # Identify the rainfall column (assuming it's the second column)\n",
    "        rainfall_col = df_new.columns[1]\n",
    "        \n",
    "        # Calculate rolling stats - using previous years' data\n",
    "        # These features help the model recognize patterns leading to extreme values\n",
    "        df_new['rolling_mean_3yr'] = df_new[rainfall_col].rolling(window=3, min_periods=1).mean().shift(1)\n",
    "        df_new['rolling_std_3yr'] = df_new[rainfall_col].rolling(window=3, min_periods=1).std().shift(1)\n",
    "        df_new['rolling_min_3yr'] = df_new[rainfall_col].rolling(window=3, min_periods=1).min().shift(1)\n",
    "        df_new['rolling_max_3yr'] = df_new[rainfall_col].rolling(window=3, min_periods=1).max().shift(1)\n",
    "        \n",
    "        # Fill NA values with appropriate statistics\n",
    "        df_new['rolling_mean_3yr'] = df_new['rolling_mean_3yr'].fillna(df_new[rainfall_col].mean())\n",
    "        df_new['rolling_std_3yr'] = df_new['rolling_std_3yr'].fillna(df_new[rainfall_col].std())\n",
    "        df_new['rolling_min_3yr'] = df_new['rolling_min_3yr'].fillna(df_new[rainfall_col].min())\n",
    "        df_new['rolling_max_3yr'] = df_new['rolling_max_3yr'].fillna(df_new[rainfall_col].max())\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Loop through each model configuration\n",
    "for _, row in model_df.iterrows():\n",
    "    station = row[\"Station\"]\n",
    "    model_name = row[\"Model\"]\n",
    "\n",
    "    # Check if station exists\n",
    "    if station not in filtered_sheet_dict:\n",
    "        print(f\"Station {station} not found in filtered data. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df = filtered_sheet_dict[station].copy()\n",
    "\n",
    "    # Ensure YEAR is treated as a simple integer for proper chronological sorting\n",
    "    if isinstance(df['YEAR'].iloc[0], str):\n",
    "        df['YEAR'] = pd.to_numeric(df['YEAR'], errors='coerce')\n",
    "    \n",
    "    # Add El Niño indicator and stronger signal for recent ones\n",
    "    df['is_el_nino'] = df['YEAR'].apply(lambda x: 1 if x in el_nino_years else 0)\n",
    "    \n",
    "    # Add more importance to recent El Niño events which were stronger\n",
    "    df['recent_strong_el_nino'] = df['YEAR'].apply(lambda x: 2 if x in [2019, 2023] else (1 if x == 2013 else 0))\n",
    "    \n",
    "    # Apply feature engineering to help with extreme values\n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    # Sort by year to ensure chronological order\n",
    "    df = df.sort_values('YEAR')\n",
    "    \n",
    "    # Print year range for verification\n",
    "    print(f\"\\nWorking with {station} data from {df['YEAR'].min()} to {df['YEAR'].max()}\")\n",
    "    print(f\"Total data points: {len(df)}\")\n",
    "\n",
    "    # Log feature names for reference\n",
    "    feature_columns = [col for col in df.columns if col != 'YEAR' and col != df.columns[1]]\n",
    "    print(f\"Features used: {', '.join(feature_columns)}\")\n",
    "\n",
    "    target_col = df.columns[1]  # Assuming this is the rainfall column\n",
    "    \n",
    "    # Save years separately before dropping for plotting\n",
    "    years = df['YEAR'].values\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df.drop(columns=[\"YEAR\", target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Skip if no features\n",
    "    if X.shape[1] == 0:\n",
    "        print(f\"No features available for {station}. Skipping model creation.\")\n",
    "        continue\n",
    "\n",
    "    # Create model with appropriate hyperparameters for extreme value capture\n",
    "    if model_name == \"Random Forest\":\n",
    "        # Deeper trees and more estimators to capture complex patterns\n",
    "        model = Pipeline([\n",
    "            ('scaler', RobustScaler()),  # Robust to outliers\n",
    "            ('model', RandomForestRegressor(\n",
    "                n_estimators=200, \n",
    "                max_depth=None,  # Let trees grow deeper\n",
    "                min_samples_leaf=1,  # Allow leaf nodes with just 1 sample\n",
    "                min_samples_split=2,\n",
    "                bootstrap=True,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    elif model_name == \"Gradient Boosting\":\n",
    "        # Use a huber loss to be more robust to outliers\n",
    "        model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', GradientBoostingRegressor(\n",
    "                loss='huber',  # More robust to outliers than squared error\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.05,  # Slower learning rate to avoid overfitting\n",
    "                max_depth=6,\n",
    "                min_samples_leaf=3,\n",
    "                subsample=0.8,  # Use 80% of samples in each boosting iteration\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    elif model_name == \"SVR\":\n",
    "        # Use RBF kernel with balanced C and epsilon parameters\n",
    "        model = Pipeline([\n",
    "            ('scaler', StandardScaler()),  # SVR needs scaling\n",
    "            ('model', SVR(\n",
    "                kernel='rbf',\n",
    "                C=10.0,        # Higher C gives more weight to outliers\n",
    "                epsilon=0.1,   # Smaller epsilon allows more support vectors\n",
    "                gamma='scale' \n",
    "            ))\n",
    "        ])\n",
    "    elif model_name == \"Neural Network\":\n",
    "        # Deeper network with dropout for regularization\n",
    "        model = Pipeline([\n",
    "            ('scaler', StandardScaler()),  # Neural networks need scaling\n",
    "            ('model', MLPRegressor(\n",
    "                hidden_layer_sizes=(100, 50, 25),  # Deeper architecture\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                alpha=0.0001,  # L2 regularization parameter\n",
    "                batch_size='auto',\n",
    "                learning_rate='adaptive',\n",
    "                max_iter=2000,\n",
    "                early_stopping=True,  # Use early stopping to prevent overfitting\n",
    "                validation_fraction=0.1,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "    else:\n",
    "        print(f\"Unknown model type: {model_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Skip if not enough data\n",
    "    if len(X) < 5:\n",
    "        print(f\"Not enough data points for {station}. Skipping model creation.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate split indices based on your desired percentages\n",
    "    train_size = 0.75  # 50% for training\n",
    "    val_size = 0.15    # 30% for validation\n",
    "    test_size = 0.1   # 20% for testing\n",
    "\n",
    "    # Calculate split points\n",
    "    train_end = int(len(X) * train_size)\n",
    "    val_end = train_end + int(len(X) * val_size)\n",
    "\n",
    "    # Print the number of samples in each split\n",
    "    print(f\"Training samples: {train_end}\")\n",
    "    print(f\"Validation samples: {val_end - train_end}\")\n",
    "    print(f\"Testing samples: {len(X) - val_end}\")\n",
    "\n",
    "    # Split data while preserving time order\n",
    "    X_train = X.iloc[:train_end]\n",
    "    y_train = y.iloc[:train_end]\n",
    "    years_train = years[:train_end]\n",
    "\n",
    "    X_val = X.iloc[train_end:val_end]\n",
    "    y_val = y.iloc[train_end:val_end]\n",
    "    years_val = years[train_end:val_end]\n",
    "\n",
    "    X_test = X.iloc[val_end:]\n",
    "    y_test = y.iloc[val_end:]\n",
    "    years_test = years[val_end:]\n",
    "\n",
    "    # Print year ranges for verification\n",
    "    print(f\"Training years: {min(years_train)} to {max(years_train)}\")\n",
    "    print(f\"Validation years: {min(years_val)} to {max(years_val)}\")\n",
    "    print(f\"Testing years: {min(years_test)} to {max(years_test)}\")\n",
    "\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on validation and test sets\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        mse = mean_squared_error(y_test, y_test_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "        mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        \n",
    "        # Special metric for extreme values: Calculate error on values below zero\n",
    "        below_zero_indices = np.where(y_test < 0)[0]\n",
    "        if len(below_zero_indices) > 0:\n",
    "            below_zero_rmse = np.sqrt(mean_squared_error(\n",
    "                y_test.iloc[below_zero_indices], \n",
    "                y_test_pred[below_zero_indices]\n",
    "            ))\n",
    "            print(f\"  RMSE for values below zero: {below_zero_rmse:.3f}\")\n",
    "            print(f\"  Number of below-zero actual values in test set: {len(below_zero_indices)}\")\n",
    "        else:\n",
    "            below_zero_rmse = None\n",
    "            print(\"  No below-zero values in test set to evaluate\")\n",
    "        \n",
    "        # Store results\n",
    "        results_dict.setdefault(station, {})[model_name] = {\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mae': mae,\n",
    "            'below_zero_rmse': below_zero_rmse,\n",
    "            'predictions': y_test_pred,\n",
    "            'actuals': y_test.values,\n",
    "            'years': years_test\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nResults for {station} using {model_name}:\")\n",
    "        print(f\"  R² = {r2:.3f}\")\n",
    "        print(f\"  RMSE = {rmse:.3f}\")\n",
    "        print(f\"  MAE = {mae:.3f}\")\n",
    "        \n",
    "        # Print validation metrics\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        print(f\"  Validation RMSE = {val_rmse:.3f}\")\n",
    "        \n",
    "        # Plot with proper x-axis formatting and focus on extremes\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Create a more detailed visualization\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(years_test, y_test.values, label=\"Observed\", marker='o', color='blue')\n",
    "        plt.plot(years_test, y_test_pred, label=\"Predicted\", linestyle='--', marker='x', color='red')\n",
    "        \n",
    "        # Add a horizontal line at y=0 to highlight the zero threshold\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        plt.title(f\"{station} - {model_name}: Observed vs Predicted (Test Set)\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(f\"{target_col}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Ensure x-axis shows actual years\n",
    "        plt.xticks(years_test)\n",
    "        if len(years_test) > 6:\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add a second subplot focusing on error analysis\n",
    "        plt.subplot(2, 1, 2)\n",
    "        errors = y_test.values - y_test_pred\n",
    "        plt.bar(years_test, errors, color='purple', alpha=0.6)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.title(\"Prediction Errors (Actual - Predicted)\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.grid(True, axis='y')\n",
    "        \n",
    "        # Ensure x-axis shows actual years\n",
    "        plt.xticks(years_test)\n",
    "        if len(years_test) > 6:\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Create a scatterplot to evaluate how well the model predicts across the range\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test.values, y_test_pred, alpha=0.6)\n",
    "        \n",
    "        # Add perfect prediction line\n",
    "        min_val = min(min(y_test.values), min(y_test_pred))\n",
    "        max_val = max(max(y_test.values), max(y_test_pred))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "        \n",
    "        # Add vertical line at y=0\n",
    "        plt.axvline(x=0, color='red', linestyle='--', alpha=0.3)\n",
    "        # Add horizontal line at y=0\n",
    "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.title(f\"{station} - {model_name}: Actual vs Predicted Values\")\n",
    "        plt.xlabel(\"Actual Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name} for {station}: {str(e)}\")\n",
    "\n",
    "# After running all models, create a summary dataframe of best models\n",
    "summary_rows = []\n",
    "\n",
    "for station in results_dict:\n",
    "    station_models = results_dict[station]\n",
    "    \n",
    "    # Find the best model based on R² score\n",
    "    best_model_name = max(station_models.keys(), key=lambda k: station_models[k]['r2'])\n",
    "    best_model = station_models[best_model_name]\n",
    "    \n",
    "    # Get features used\n",
    "    features_used = ', '.join(feature_columns)\n",
    "    \n",
    "    # Add to summary\n",
    "    summary_rows.append({\n",
    "        'Station': station,\n",
    "        'Best Model': best_model_name,\n",
    "        'R2': best_model['r2'],\n",
    "        'RMSE': best_model['rmse'],\n",
    "        'MAE': best_model['mae'],\n",
    "        'Features Used': features_used\n",
    "    })\n",
    "\n",
    "# Create and display summary dataframe\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Summary of Best Models for Each Station\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=True))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca4c8e-2d3c-4da9-9110-3ad4667478db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dataframe to store the best models and their performance metrics\n",
    "best_models_summary = pd.DataFrame(columns=[\n",
    "    'Station', 'Best Model', 'R2', 'RMSE', 'MAE', 'Features Used'\n",
    "])\n",
    "\n",
    "# Create a dictionary to store forecasting-ready dataframes for each station\n",
    "forecasting_dfs = {}\n",
    "\n",
    "# Process results from cell 4 to identify the best model for each station\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL SELECTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for station, models_dict in results_dict.items():\n",
    "    print(f\"\\nAnalyzing models for station: {station}\")\n",
    "    \n",
    "    best_model_name = None\n",
    "    best_r2 = float('-inf')\n",
    "    best_metrics = {}\n",
    "\n",
    "    # Find the model with the highest R² value from the results_dict (from cell 4)\n",
    "    for model_name, metrics in models_dict.items():\n",
    "        r2 = metrics['r2']\n",
    "        print(f\"  {model_name}: R² = {r2:.4f}, RMSE = {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model_name = model_name\n",
    "            best_metrics = metrics\n",
    "    \n",
    "    if best_model_name:\n",
    "        try:\n",
    "            print(f\"\\n✅ Best model for {station}: {best_model_name}\")\n",
    "            print(f\"  R² = {best_r2:.4f}\")\n",
    "            print(f\"  RMSE = {best_metrics['rmse']:.4f}\")\n",
    "            print(f\"  MAE = {best_metrics['mae']:.4f}\")\n",
    "            \n",
    "            # Get the original dataframe for this station\n",
    "            station_df = filtered_sheet_dict[station].copy()\n",
    "            \n",
    "            # Get the features used for this station - use exactly the same features from cell 4\n",
    "            features_used = [col for col in station_df.columns if col not in ['YEAR', station]]\n",
    "            features_str = ', '.join(features_used)\n",
    "            \n",
    "            # Add El Niño indicator for future forecasting\n",
    "            el_nino_years = [1997, 2013, 2019, 2023]\n",
    "            station_df['is_el_nino'] = station_df['YEAR'].apply(lambda x: 1 if int(x) in el_nino_years else 0)\n",
    "            \n",
    "            best_models_summary = pd.concat([\n",
    "                best_models_summary,\n",
    "                pd.DataFrame({\n",
    "                    'Station': [station],\n",
    "                    'Best Model': [best_model_name],\n",
    "                    'R2': [best_r2],\n",
    "                    'RMSE': [best_metrics['rmse']],\n",
    "                    'MAE': [best_metrics['mae']],\n",
    "                    'Features Used': [features_str]\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # Store the original dataframe with the best model predictions for forecasting\n",
    "            forecasting_dfs[station] = {\n",
    "                'df': station_df,\n",
    "                'model_name': best_model_name,\n",
    "                'features': features_used,\n",
    "                'target': station,\n",
    "                'years': best_metrics['years'],\n",
    "                'actuals': best_metrics['actuals'],\n",
    "                'predictions': best_metrics['predictions'],\n",
    "                'r2': best_r2,\n",
    "                'rmse': best_metrics['rmse'],\n",
    "                'mae': best_metrics['mae']\n",
    "            }\n",
    "\n",
    "            \n",
    "            years_test = best_metrics['years']\n",
    "            y_test = best_metrics['actuals']\n",
    "            y_pred = best_metrics['predictions']\n",
    "            target_col = station\n",
    "\n",
    "            print(\"Years in test set:\", years_test)\n",
    "\n",
    "            if len(set(years_test)) == 1:\n",
    "                print(\"WARNING: All test data is from the same year!\")\n",
    "\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            if len(set(years_test)) == 1:\n",
    "                x_vals = np.arange(len(y_test))\n",
    "                plt.plot(x_vals, y_test, label=\"Observed\", marker='o')\n",
    "                plt.plot(x_vals, y_pred, label=\"Predicted\", linestyle='--', marker='x')\n",
    "                plt.xticks(x_vals, [years_test[0]] * len(y_test))\n",
    "            else:\n",
    "                plt.plot(years_test, y_test, label=\"Observed\", marker='o')\n",
    "                plt.plot(years_test, y_pred, label=\"Predicted\", linestyle='--', marker='x')\n",
    "\n",
    "            plt.title(f\"{station} - {best_model_name}: Observed vs Predicted (Test Set)\")\n",
    "            plt.xlabel(\"Year\")\n",
    "            plt.ylabel(f\"{target_col}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling best model for {station}: {str(e)}\")\n",
    "            \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODELS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(best_models_summary)\n",
    "\n",
    "print(\"\\nPrepared forecasting dataframes for these stations:\")\n",
    "for station in forecasting_dfs.keys():\n",
    "    print(f\"- {station}\")\n",
    "\n",
    "print(\"\\nForecasting-ready dataframes and best models are stored in 'forecasting_dfs'\")\n",
    "print(\"You can use these in cell 6 for drought prediction and forecasting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1b1af-986d-4df3-b4c4-ac960442ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecasting_dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b68159-6c70-4001-8c61-c850c0eddb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import traceback\n",
    "\n",
    "# Define drought threshold\n",
    "DROUGHT_THRESHOLD = -0.5\n",
    "\n",
    "# Dictionary to store results for all stations\n",
    "drought_analysis = {}\n",
    "\n",
    "# First, make sure we have the forecasting dataframes\n",
    "# This assumes you have already created a forecasting_dfs dictionary from your model results\n",
    "# If not, let's create it based on your best models\n",
    "\n",
    "# Get the list of stations from the results_dict\n",
    "stations = list(results_dict.keys())\n",
    "\n",
    "# Create a base forecasting dataframe with years\n",
    "# Assuming we want to analyze the test period data first\n",
    "forecasting_df = pd.DataFrame()\n",
    "forecasting_df['YEAR'] = np.array([])  # Will be populated below\n",
    "\n",
    "# Now let's analyze each station\n",
    "for station in stations:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{station} STATION DROUGHT ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get the best model results for this station\n",
    "    best_model_name = max(results_dict[station].keys(), key=lambda k: results_dict[station][k]['r2'])\n",
    "    station_results = results_dict[station][best_model_name]\n",
    "    \n",
    "    # Get years and actual rainfall values from test set\n",
    "    years = station_results['years']\n",
    "    rainfall = station_results['actuals']  # Actual values\n",
    "    \n",
    "    # Add to forecasting dataframe if it's empty\n",
    "    if len(forecasting_df['YEAR']) == 0:\n",
    "        forecasting_df['YEAR'] = years\n",
    "    \n",
    "    # Add station data to forecasting dataframe\n",
    "    forecasting_df[station] = rainfall\n",
    "    \n",
    "    # Create container for this station's analysis\n",
    "    drought_analysis[station] = {}\n",
    "\n",
    "    # Step 1: Identify drought years\n",
    "    drought_years = years[rainfall <= DROUGHT_THRESHOLD]\n",
    "    drought_frequency = len(drought_years) / len(years) * 100\n",
    "\n",
    "    print(f\"\\nHistorical drought years: {drought_years}\")\n",
    "    print(f\"Historical drought frequency: {drought_frequency:.1f}%\")\n",
    "\n",
    "    # Store results\n",
    "    drought_analysis[station]['historical_drought_years'] = drought_years.tolist() if hasattr(drought_years, 'tolist') else list(drought_years)\n",
    "    drought_analysis[station]['historical_frequency'] = drought_frequency\n",
    "\n",
    "    # Step 2: Compute average drought interval if applicable\n",
    "    if len(drought_years) > 1:\n",
    "        intervals = np.diff(drought_years)\n",
    "        avg_interval = np.mean(intervals)\n",
    "        print(f\"\\nAverage interval between droughts: {avg_interval:.1f} years\")\n",
    "        drought_analysis[station]['avg_interval'] = float(avg_interval)\n",
    "    else:\n",
    "        avg_interval = None\n",
    "        print(\"\\nNot enough drought events to compute average interval.\")\n",
    "        drought_analysis[station]['avg_interval'] = None\n",
    "\n",
    "    # Step 3: Forecast future droughts using ARIMA\n",
    "    try:\n",
    "        # Prepare time series\n",
    "        # We need a longer series for ARIMA, so let's use all available data\n",
    "        # For demonstration, I'll just use the test set data but in practice you should use your full historical dataset\n",
    "        ts_data = pd.Series(rainfall, index=years)\n",
    "        ts_data.name = 'Rainfall'\n",
    "        ts_data = ts_data.dropna()\n",
    "\n",
    "        # Check if we have enough data points\n",
    "        if len(ts_data) < 8:  # Lowered from 20 to 8 for demonstration\n",
    "            raise ValueError(f\"Not enough non-NaN data points for ARIMA (only {len(ts_data)} available).\")\n",
    "\n",
    "        # Fit ARIMA model (p=1, d=1, q=1)\n",
    "        # In production, you might want to use auto_arima to find the best parameters\n",
    "        model = ARIMA(ts_data, order=(1, 1, 0))  # Simplified to (1,1,0) for better stability\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Forecast next 10 years\n",
    "        forecast_years = np.arange(ts_data.index[-1] + 1, ts_data.index[-1] + 11)\n",
    "        forecast = model_fit.forecast(steps=10)\n",
    "\n",
    "        # Identify predicted drought years\n",
    "        forecast_drought_flags = forecast <= DROUGHT_THRESHOLD\n",
    "        predicted_drought_years = forecast_years[forecast_drought_flags]\n",
    "\n",
    "        print(\"\\nPredicted rainfall values for the next 10 years:\")\n",
    "        for year, value in zip(forecast_years, forecast):\n",
    "            drought_status = \"DROUGHT PREDICTED\" if value <= DROUGHT_THRESHOLD else \"No drought\"\n",
    "            print(f\"{year}: {value:.3f} ({drought_status})\")\n",
    "\n",
    "        # Store forecast results\n",
    "        drought_analysis[station]['forecast_years'] = forecast_years.tolist() if hasattr(forecast_years, 'tolist') else list(forecast_years)\n",
    "        drought_analysis[station]['forecast_values'] = forecast.tolist() if hasattr(forecast, 'tolist') else list(forecast)\n",
    "        drought_analysis[station]['predicted_drought_years'] = predicted_drought_years.tolist() if hasattr(predicted_drought_years, 'tolist') else list(predicted_drought_years)\n",
    "\n",
    "        if len(predicted_drought_years) > 0:\n",
    "            next_drought = predicted_drought_years[0]\n",
    "            print(f\"\\nNext predicted drought for {station}: {next_drought}\")\n",
    "            years_until_drought = next_drought - ts_data.index[-1]\n",
    "            print(f\"Years until next drought: {years_until_drought}\")\n",
    "            drought_analysis[station]['next_drought_year'] = int(next_drought)\n",
    "            drought_analysis[station]['years_until_drought'] = int(years_until_drought)\n",
    "        else:\n",
    "            print(f\"\\nNo droughts predicted for {station} in the next 10 years\")\n",
    "            drought_analysis[station]['next_drought_year'] = None\n",
    "            drought_analysis[station]['years_until_drought'] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in ARIMA forecast for {station}: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Falling back to pattern-based estimation...\")\n",
    "\n",
    "        # Fallback to historical pattern if possible\n",
    "        if avg_interval is not None and len(drought_years) > 0:\n",
    "            last_drought = drought_years[-1]\n",
    "            pattern_next_drought = int(last_drought + avg_interval)\n",
    "            print(f\"Based on historical patterns (avg interval: {avg_interval:.1f} years):\")\n",
    "            print(f\"Next expected drought around: {pattern_next_drought}\")\n",
    "            drought_analysis[station]['pattern_next_drought'] = pattern_next_drought\n",
    "        else:\n",
    "            print(\"No pattern-based prediction available due to insufficient data.\")\n",
    "            drought_analysis[station]['pattern_next_drought'] = None\n",
    "\n",
    "# Create visualizations for drought analysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot historical droughts and predictions for each station\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, station in enumerate(stations):\n",
    "    plt.subplot(len(stations), 1, i+1)\n",
    "    \n",
    "    # Get results for this station\n",
    "    results = drought_analysis[station]\n",
    "    \n",
    "    # Plot historical data points\n",
    "    years = forecasting_df['YEAR']\n",
    "    values = forecasting_df[station]\n",
    "    \n",
    "    # Plot the full rainfall line\n",
    "    plt.plot(years, values, 'b-', label='Historical')\n",
    "    \n",
    "    # Highlight drought points\n",
    "    drought_mask = values <= DROUGHT_THRESHOLD\n",
    "    if any(drought_mask):\n",
    "        plt.scatter(years[drought_mask], values[drought_mask], color='red', s=50, label='Historical Droughts')\n",
    "    \n",
    "    # Plot forecast if available\n",
    "    if 'forecast_years' in results and 'forecast_values' in results:\n",
    "        forecast_years = results['forecast_years']\n",
    "        forecast_values = results['forecast_values']\n",
    "        plt.plot(forecast_years, forecast_values, 'g--', label='ARIMA Forecast')\n",
    "        \n",
    "        # Highlight predicted droughts\n",
    "        forecast_drought_mask = np.array(forecast_values) <= DROUGHT_THRESHOLD\n",
    "        if any(forecast_drought_mask):\n",
    "            plt.scatter(\n",
    "                np.array(forecast_years)[forecast_drought_mask], \n",
    "                np.array(forecast_values)[forecast_drought_mask], \n",
    "                color='red', marker='x', s=50, label='Predicted Droughts'\n",
    "            )\n",
    "    \n",
    "    # Add pattern-based prediction if available\n",
    "    if results.get('pattern_next_drought') is not None:\n",
    "        pattern_year = results['pattern_next_drought']\n",
    "        plt.axvline(x=pattern_year, color='purple', linestyle='--', \n",
    "                    label=f'Pattern-based ({pattern_year})')\n",
    "    \n",
    "    # Add drought threshold line\n",
    "    plt.axhline(y=DROUGHT_THRESHOLD, color='r', linestyle='-', alpha=0.3, \n",
    "                label=f'Drought Threshold ({DROUGHT_THRESHOLD})')\n",
    "    \n",
    "    plt.title(f\"{station} - Drought Analysis\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Rainfall Anomaly\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of Drought Predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY OF DROUGHT PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "for station, results in drought_analysis.items():\n",
    "    print(f\"\\n{station}:\")\n",
    "    print(f\"  Historical drought frequency: {results['historical_frequency']:.1f}%\")\n",
    "\n",
    "    next_drought = \"None predicted\"\n",
    "    method = \"N/A\"\n",
    "    \n",
    "    if results.get('next_drought_year') is not None:\n",
    "        next_drought = results['next_drought_year']\n",
    "        years_until = results['years_until_drought']\n",
    "        print(f\"  Next drought (ARIMA-based): {next_drought} ({years_until} years from last data point)\")\n",
    "        method = \"ARIMA\"\n",
    "    elif results.get('pattern_next_drought') is not None:\n",
    "        next_drought = results['pattern_next_drought']\n",
    "        print(f\"  Next drought (Pattern-based): {next_drought}\")\n",
    "        method = \"Pattern\"\n",
    "    else:\n",
    "        print(f\"  No drought prediction available\")\n",
    "    \n",
    "    # Add to summary data\n",
    "    summary_data.append({\n",
    "        'Station': station,\n",
    "        'Historical Frequency (%)': results['historical_frequency'],\n",
    "        'Next Drought Year': next_drought,\n",
    "        'Prediction Method': method\n",
    "    })\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nDrought Prediction Summary Table:\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a28a1-caba-4aa9-894e-9a6e64a37651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
